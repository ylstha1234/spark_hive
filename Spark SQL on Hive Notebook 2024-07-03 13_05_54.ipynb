{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbbfea7-a775-4e5c-9402-b98be1075b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing the pyspark and pyspark SQL modules and specifying the app name \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "appName= \"spark hive example\"\n",
    "master= \"local\"\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7abad7-2ef4-49bb-a447-f6223e84a841",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n||\n++\n++\n\n+-------------------------+---------------------------------------+\n|database_description_item|database_description_value             |\n+-------------------------+---------------------------------------+\n|Catalog Name             |spark_catalog                          |\n|Namespace Name           |spark_hive                             |\n|Comment                  |                                       |\n|Location                 |dbfs:/user/hive/warehouse/spark_hive.db|\n|Owner                    |root                                   |\n+-------------------------+---------------------------------------+\n\n++\n||\n++\n++\n\n"
     ]
    }
   ],
   "source": [
    "# Creating a spark session and enabling the Hive support to interact with the Hive database\n",
    "spark = SparkSession.builder \\\n",
    "\t.master(master).appName(appName).enableHiveSupport().getOrCreate() \n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_hive\").show()\n",
    "spark.sql(\"DESCRIBE DATABASE spark_hive\").show(truncate=False)\n",
    "spark.sql(\"USE spark_hive\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f86f4c4-2087-4bd6-9a0d-e07cfd9abb12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|databaseName|\n+------------+\n|     default|\n|  spark_hive|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verifying the databases in Hive using pyspark\n",
    "df=spark.sql(\"show databases\")\n",
    "df.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb95214-621c-47b6-bc08-a4461b0a02c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+--------+--------------+--------------------+-----------------+-------------+------+---------+-------+-------------+\n|EmployeeID|ManagerID|FirstName|LastName|      FullName|            JobTitle|OrganizationLevel|MaritalStatus|Gender|Territory|Country|        Group|\n+----------+---------+---------+--------+--------------+--------------------+-----------------+-------------+------+---------+-------+-------------+\n|       274|     NULL|  Stephen|   Jiang| Stephen Jiang|North American Sa...|                2|            M|     M|     NULL|   NULL|         NULL|\n|       275|      274|  Michael|  Blythe|Michael Blythe|Sales Representative|                3|            S|     M|Northeast|     US|North America|\n|       276|      274|    Linda|Mitchell|Linda Mitchell|Sales Representative|                3|            M|     F|Southwest|     US|North America|\n|       277|      274|  Jillian|  Carson|Jillian Carson|Sales Representative|                3|            S|     F|  Central|     US|North America|\n|       278|      274|  Garrett|  Vargas|Garrett Vargas|Sales Representative|                3|            M|     M|   Canada|     CA|North America|\n+----------+---------+---------+--------+--------------+--------------------+-----------------+-------------+------+---------+-------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Specifying the path to your CSV file\n",
    "csv_file_path = \"dbfs:/FileStore/tables/employees.csv\"\n",
    "\n",
    "# Reading CSV file into a DataFrame\n",
    "datafile = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# Showing the first 5 rows to verify the data is read correctly\n",
    "datafile.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990d8012-9937-4eb4-90d3-beaa44ec6cd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving DataFrame as Hive table\n",
    "datafile.write.saveAsTable(\"employees_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61aa7617-0f97-49a3-b8ec-f59c223e480b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+--------+--------------+--------------------+-----------------+-------------+------+---------+-------+-------------+\n|EmployeeID|ManagerID|FirstName|LastName|      FullName|            JobTitle|OrganizationLevel|MaritalStatus|Gender|Territory|Country|        Group|\n+----------+---------+---------+--------+--------------+--------------------+-----------------+-------------+------+---------+-------+-------------+\n|       274|     NULL|  Stephen|   Jiang| Stephen Jiang|North American Sa...|                2|            M|     M|     NULL|   NULL|         NULL|\n|       275|      274|  Michael|  Blythe|Michael Blythe|Sales Representative|                3|            S|     M|Northeast|     US|North America|\n|       276|      274|    Linda|Mitchell|Linda Mitchell|Sales Representative|                3|            M|     F|Southwest|     US|North America|\n|       277|      274|  Jillian|  Carson|Jillian Carson|Sales Representative|                3|            S|     F|  Central|     US|North America|\n|       278|      274|  Garrett|  Vargas|Garrett Vargas|Sales Representative|                3|            M|     M|   Canada|     CA|North America|\n+----------+---------+---------+--------+--------------+--------------------+-----------------+-------------+------+---------+-------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Querying the Hive table using Spark SQL\n",
    "result = spark.sql(\"SELECT * FROM employees\")\n",
    "\n",
    "# Showing the query result\n",
    "result.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aada9567-4879-4a23-b49c-43b1b8fb06af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+------------+-------------------+--------------------+-----------------+-------------+------+---------+-------+-------------+\n|EmployeeID|ManagerID|FirstName|    LastName|           FullName|            JobTitle|OrganizationLevel|MaritalStatus|Gender|Territory|Country|        Group|\n+----------+---------+---------+------------+-------------------+--------------------+-----------------+-------------+------+---------+-------+-------------+\n|       276|      274|    Linda|    Mitchell|     Linda Mitchell|Sales Representative|                3|            M|     F|Southwest|     US|North America|\n|       277|      274|  Jillian|      Carson|     Jillian Carson|Sales Representative|                3|            S|     F|  Central|     US|North America|\n|       278|      274|  Garrett|      Vargas|     Garrett Vargas|Sales Representative|                3|            M|     M|   Canada|     CA|North America|\n|       279|      274|     Tsvi|      Reiter|        Tsvi Reiter|Sales Representative|                3|            M|     M|Southeast|     US|North America|\n|       280|      274|   Pamela|Ansman-Wolfe|Pamela Ansman-Wolfe|Sales Representative|                3|            S|     F|Northwest|     US|North America|\n+----------+---------+---------+------------+-------------------+--------------------+-----------------+-------------+------+---------+-------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Querying the Hive table\n",
    "result = spark.sql(\"SELECT * FROM employees_table WHERE EmployeeID > 275\")\n",
    "result.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark SQL on Hive Notebook 2024-07-03 13:05:54",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
