{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbbfea7-a775-4e5c-9402-b98be1075b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing the pyspark and pyspark SQL modules and specifying the app name \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "appName= \"Spark Cleansing\"\n",
    "master= \"local\"\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7abad7-2ef4-49bb-a447-f6223e84a841",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n||\n++\n++\n\n+-------------------------+-------------------------------------------------+\n|database_description_item|database_description_value                       |\n+-------------------------+-------------------------------------------------+\n|Catalog Name             |spark_catalog                                    |\n|Namespace Name           |spark_data_cleansing                             |\n|Comment                  |                                                 |\n|Location                 |dbfs:/user/hive/warehouse/spark_data_cleansing.db|\n|Owner                    |root                                             |\n+-------------------------+-------------------------------------------------+\n\n++\n||\n++\n++\n\n"
     ]
    }
   ],
   "source": [
    "# Creating a spark session and enabling the Hive support to interact with the Hive database\n",
    "spark = SparkSession.builder \\\n",
    "\t.master(master).appName(appName).enableHiveSupport().getOrCreate() \n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_data_cleansing\").show()\n",
    "spark.sql(\"DESCRIBE DATABASE spark_data_cleansing\").show(truncate=False)\n",
    "spark.sql(\"USE spark_data_cleansing\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f86f4c4-2087-4bd6-9a0d-e07cfd9abb12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|        databaseName|\n+--------------------+\n|             default|\n|spark_data_cleansing|\n|          spark_hive|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verifying the databases in Hive using pyspark\n",
    "df=spark.sql(\"show databases\")\n",
    "df.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb95214-621c-47b6-bc08-a4461b0a02c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---+-------+\n|CustomerID|        FullName|Age| Salary|\n+----------+----------------+---+-------+\n|       293|  Catherine Abel| 50| 500000|\n|       295| Kim Abercrombie| 30| 600000|\n|       297|Humberto Acevedo| 55|   None|\n|       293|  Catherine Abel| 50| 500000|\n|       299|  Pilar Ackerman| 35|5000000|\n|       305|     Carla Adams| 42| 300000|\n|       301|   Frances Adams| 34| 250000|\n|       307|       Jay Adams| 30| 210000|\n|       305|     Carla Adams| 42| 300000|\n|       311| Samuel Agcaoili| 21|   None|\n+----------+----------------+---+-------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Specifying the path to your CSV file\n",
    "csv_file_path = \"dbfs:/FileStore/tables/CUSTOMERS.csv\"\n",
    "\n",
    "# Reading CSV file into a DataFrame\n",
    "datafile = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# Showing the first 5 rows to verify the data is read correctly\n",
    "datafile.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990d8012-9937-4eb4-90d3-beaa44ec6cd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving DataFrame as Hive table\n",
    "datafile.write.saveAsTable(\"CustomerTBL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61aa7617-0f97-49a3-b8ec-f59c223e480b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---+-------+\n|CustomerID|        FullName|Age| Salary|\n+----------+----------------+---+-------+\n|       293|  Catherine Abel| 50| 500000|\n|       295| Kim Abercrombie| 30| 600000|\n|       297|Humberto Acevedo| 55|   None|\n|       293|  Catherine Abel| 50| 500000|\n|       299|  Pilar Ackerman| 35|5000000|\n|       305|     Carla Adams| 42| 300000|\n|       301|   Frances Adams| 34| 250000|\n|       307|       Jay Adams| 30| 210000|\n|       305|     Carla Adams| 42| 300000|\n|       311| Samuel Agcaoili| 21|   None|\n+----------+----------------+---+-------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Querying the Hive table using Spark SQL\n",
    "df = spark.sql(\"SELECT * FROM CustomerTBL\")\n",
    "\n",
    "# Showing the query result\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aada9567-4879-4a23-b49c-43b1b8fb06af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- CustomerID: string (nullable = true)\n |-- FullName: string (nullable = true)\n |-- Age: string (nullable = true)\n |-- Salary: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Showing schema\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285c3362-9bba-4902-992b-ddefc0228835",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---+---------+\n|CustomerID|        FullName|Age|   Salary|\n+----------+----------------+---+---------+\n|       293|  Catherine Abel| 50| 500000.0|\n|       295| Kim Abercrombie| 30| 600000.0|\n|       297|Humberto Acevedo| 55| 715625.0|\n|       293|  Catherine Abel| 50| 500000.0|\n|       299|  Pilar Ackerman| 35|5000000.0|\n|       305|     Carla Adams| 42| 300000.0|\n|       301|   Frances Adams| 34| 250000.0|\n|       307|       Jay Adams| 30| 210000.0|\n|       305|     Carla Adams| 42| 300000.0|\n|       311| Samuel Agcaoili| 21| 715625.0|\n+----------+----------------+---+---------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Values\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Converting Age and Salary columns to correct types\n",
    "df = df.withColumn(\"Age\", df[\"Age\"].cast(\"long\"))\n",
    "df = df.withColumn(\"Salary\", df[\"Salary\"].cast(\"double\"))\n",
    "\n",
    "# Calculating means\n",
    "mean_age = df.agg(mean(col(\"Age\"))).first()[0]\n",
    "mean_salary = df.agg(mean(col(\"Salary\"))).first()[0]\n",
    "\n",
    "# Replacing null values with means\n",
    "df = df.fillna({\"Age\": mean_age, \"Salary\": mean_salary})\n",
    "\n",
    "# Displaying cleaned DataFrame\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5edbf0c-bdb1-4b95-8c41-2ba604dd5fef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data Frame:\n+----------+----------------+---+--------+\n|CustomerID|        FullName|Age|  Salary|\n+----------+----------------+---+--------+\n|       313|   James Aguilar| 23|450000.0|\n|       319|       Kim Akers| 25|720000.0|\n|       325|   Anna Albright| 51|440000.0|\n|       301|   Frances Adams| 34|250000.0|\n|       329|     Paul Alcorn| 38| 20000.0|\n|       297|Humberto Acevedo| 55|715625.0|\n|       327|   Milton Albury| 60|450000.0|\n|       305|     Carla Adams| 42|300000.0|\n|       293|  Catherine Abel| 50|500000.0|\n|       315| Robert Ahlering| 37|610000.0|\n+----------+----------------+---+--------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Handling Duplicates\n",
    "# Removing duplicates\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Displaying the cleaned data frame\n",
    "print(\"Cleaned Data Frame:\")\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2351d0bb-c218-4cc2-82cc-e7a07626e347",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data Frame:\n+----------+----------------+---+--------+\n|CustomerID|        FullName|Age|  Salary|\n+----------+----------------+---+--------+\n|       313|   James Aguilar| 23|450000.0|\n|       319|       Kim Akers| 25|720000.0|\n|       325|   Anna Albright| 51|440000.0|\n|       301|   Frances Adams| 34|250000.0|\n|       329|     Paul Alcorn| 38| 20000.0|\n|       297|Humberto Acevedo| 55|715625.0|\n|       327|   Milton Albury| 60|450000.0|\n|       305|     Carla Adams| 42|300000.0|\n|       293|  Catherine Abel| 50|500000.0|\n|       315| Robert Ahlering| 37|610000.0|\n+----------+----------------+---+--------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Handling Outliers\n",
    "from pyspark.sql.functions import mean, stddev, abs\n",
    "\n",
    "# Calculating mean and standard deviation of Salary\n",
    "agg_stats = df.agg(mean(col(\"Salary\")).alias(\"mean_salary\"), stddev(col(\"Salary\")).alias(\"stddev_salary\")).first()\n",
    "mean_salary = agg_stats[\"mean_salary\"]\n",
    "stddev_salary = agg_stats[\"stddev_salary\"]\n",
    "\n",
    "# Filtering outliers based on mean and standard deviation\n",
    "df_filtered = df.filter((col(\"Salary\") >= mean_salary - 2 * stddev_salary) & (col(\"Salary\") <= mean_salary + 2 * stddev_salary))\n",
    "\n",
    "# Displaying the cleaned DataFrame\n",
    "print(\"Cleaned Data Frame:\")\n",
    "df_filtered.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "602656d8-93d5-4081-b9b4-f87646dc064b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Data Frame:\n+----------+----------------+---+--------+\n|CustomerID|        FullName|Age|  Salary|\n+----------+----------------+---+--------+\n|       313|   James Aguilar| 23|450000.0|\n|       319|       Kim Akers| 25|720000.0|\n|       325|   Anna Albright| 51|440000.0|\n|       301|   Frances Adams| 34|250000.0|\n|       329|     Paul Alcorn| 38| 20000.0|\n|       297|Humberto Acevedo| 55|715625.0|\n|       327|   Milton Albury| 60|450000.0|\n|       305|     Carla Adams| 42|300000.0|\n|       293|  Catherine Abel| 50|500000.0|\n|       315| Robert Ahlering| 37|610000.0|\n+----------+----------------+---+--------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Converting Data Types\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# Converting the data type of the 'Age' column to integer\n",
    "df = df.withColumn(\"Age\", df[\"Age\"].cast(IntegerType()))\n",
    "\n",
    "# Converting the data type of the 'Salary' column to float\n",
    "df = df.withColumn(\"Salary\", df[\"Salary\"].cast(FloatType()))\n",
    "\n",
    "# Displaying the converted data frame\n",
    "print(\"Converted Data Frame:\")\n",
    "df.show(10)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark data cleansing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
